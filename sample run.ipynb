{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this section, we will be going through an example run of how the case study should look like. Note that many comments in this notebook are unnncessary for your work as it's there for educational purposes. We begin by import the standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #not used often here, but comes in handy when doing rough calculations to figure things out\n",
    "import pandas as  pd #the package that handles our data\n",
    "import matplotlib.pyplot as plt #remember to add in .pyplot else it won't work\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #the model used for this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we import our training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data.csv', index_col=0) #recall that index_col is the variable that we use to tell pandas which column contains the index,\n",
    "test_data = pd.read_csv('test_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEBT</th>\n",
       "      <th>YRS_IN_RESIDENT</th>\n",
       "      <th>AGE</th>\n",
       "      <th>YRS_OF_EMPLOYMENT</th>\n",
       "      <th>DTI</th>\n",
       "      <th>NUM_PREV_APP</th>\n",
       "      <th>OCCUPATION</th>\n",
       "      <th>PROVIDED_SIN</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>CREDIT_PROFILE</th>\n",
       "      <th>APPROVAL_STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.500</td>\n",
       "      <td>12</td>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>11</td>\n",
       "      <td>2733</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DEBT  YRS_IN_RESIDENT   AGE  YRS_OF_EMPLOYMENT    DTI  NUM_PREV_APP  \\\n",
       "0  500.0              1.0  23.0               0.50  0.085             0   \n",
       "1    0.0              2.0  33.0               5.50  5.500            12   \n",
       "2    0.0              9.0  19.0               4.50  3.000             0   \n",
       "3  100.0              1.0  49.0               0.25  0.250            11   \n",
       "4  300.0              8.0  39.0               0.25  0.085             0   \n",
       "\n",
       "   OCCUPATION  PROVIDED_SIN  MARRIAGE  INCOME  EDUCATION  CREDIT_PROFILE  \\\n",
       "0           1             1         2       8          7               1   \n",
       "1        1001             1         2      11          7               1   \n",
       "2           1             0         1       8          7               1   \n",
       "3        2733             1         1       4          4               1   \n",
       "4           1             1         2       4          4               1   \n",
       "\n",
       "   APPROVAL_STATUS  \n",
       "0                0  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a preview of the data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's do a simple visualization for seeing the relationship of INCOME and APPROVAL_STATUS. First, we create a pivot table by getting total approved credit cards for each income bracket in INCOME in train_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPROVAL_STATUS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INCOME</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        APPROVAL_STATUS\n",
       "INCOME                 \n",
       "1                     5\n",
       "2                     6\n",
       "3                    10\n",
       "4                    12\n",
       "5                     2\n",
       "6                    15\n",
       "7                    13\n",
       "8                    53\n",
       "9                    29\n",
       "10                   12\n",
       "11                   42\n",
       "12                    2\n",
       "13                   23\n",
       "14                   24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_vs_approval = pd.pivot_table(data=train_data, values='APPROVAL_STATUS', index='INCOME', aggfunc=np.sum) #creates a DataFrame that has the pivot table\n",
    "#data: the dataframe with all the values for pivoting\n",
    "#values: the column to sum up\n",
    "#index: the column for which we sum up values for\n",
    "#aggfunc: how one wants to 'aggregate' the values, which is np.sum in this case.\n",
    "income_vs_approval #remember that jupyter notebooks always outputs the last item in the cell, unless you assign it to a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we plot the income_vs_approval using matplotlib. Observe from above that our x values are under the INCOME column and our y values are under the APPROVAL_STATUS column. To get the values under INCOME, we use income_vs_approval.index (if one recalls further, we set index='INCOME'). To get the values under APPROVAL_STATUS, we use income_vs_approval.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e988cc5908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e988975978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array(income_vs_approval.index).reshape(-1,1) #coverting it to a numpy array then reshaping it so it'll be 2D shape instead of a 1D shape.\n",
    "y = income_vs_approval.values #the .values outputs a numpy array of shape (-1,1) already\n",
    "#the reason we're doing the coversions and reshapings above is due to how sklearn processes data for fitting/learning (it needs a two dimensional shape), as we are going for a line of best fit below.\n",
    "plt.scatter(x, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We see a general trend of approvals going up the higher income bracket you reportedly are. Now, we are going to fit a regression line on it and plot it together. First, we create an regression line using LinearRegression from sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression #import linear regression from sklearn to get our regression line\n",
    "lr = LinearRegression() #creating an instance of LinearRegression\n",
    "lr.fit(x,y) #fitting our instance of LinearRegression to our pivot table's data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we plot the line with the scatter plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e988d60898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e9889cef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y2 = lr.predict(x) #yes, we are using .predict here, but since we are \"predicting\" on the training set's x values, it will output the line of best fit's y values at those x points\n",
    "plt.plot(x,y2, c='r') #c is for changing colors of the plot, 'r' means red. \n",
    "plt.scatter(x,y) #putting two plotting functions from matplotlib like this will automatically stack them together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We now move on to using logistic regression for automating credit card approvals. Before we build the model, we know that we have some nominal categorical variables. Since we are using logistic regression, we need to one hot encode them. For simplicity, we are going to only one hot encode MARRIAGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder #import the OneHotEncoder from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We are going to one hot encode both training and testing set's MARRIAGE . The reason for that is beacuse we are going to fit the model on the \"modified\" training set (due to encoding of MARRIAGE), so the test set needs the same \"modifications\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder() #create an instance of OneHotEncoder. Remember, each instance only works for a single feature. You will need more instances for other features\n",
    "encoded_marriage_train = encoder.fit_transform(train_data['MARRIAGE'].values.reshape(-1,1)) #.values.reshape is for the same reason above, sklearn needs a two dimensional shape\n",
    "encoded_marriage_test = encoder.transform(test_data['MARRIAGE'].values.reshape(-1,1)) #we use .transform because we already \"fitted\" the encoder on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before we proceed further, let's take a look at encoded_marriage_train. Observe that the output says 'sparse matrix'. A sparse matrix is a matrix with a lot of zeros. It's used because of its memory efficiency. Since one hot encoding tends to create a very large matrix with a bunch of ones and zeros, sklearn outputs sparse matrices when you use its OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<552x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 552 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_marriage_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we are almost ready to train our model. Before we do that, we are going to select a few features to use, of course MARRIAGE is one of them since we already one hot encoded it. Also, we need to turn encoded_marriage_train and encoded_marriage_test into DataFrames to make things more consistent, as sklearn only takes in one 'type' of input for data or labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['DEBT', 'AGE', 'YRS_OF_EMPLOYMENT'] #these are the features we're going to use (plus one hot encoded MARRIAGE). We put them in a list to access it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_marriage_train = pd.DataFrame(encoded_marriage_train.todense()) #the .todense() converts the sparse matrix into a regular matrix (or array), then we put in a DataFrame\n",
    "encoded_marriage_test = pd.DataFrame(encoded_marriage_test.todense()) #same for the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We then make the training and testing sets for our model based on the 4 features. This is done by extracting selected_features from train_data and test_data then concatenating (joining) it with encoded_marriage_train and encoded_marriage_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = pd.concat([train_data[selected_features], encoded_marriage_train], axis=1) #putting the DataFrames to be concatentated in a list, then set axis=1 since we are joining them by columns\n",
    "test_data2 = pd.concat([test_data[selected_features], encoded_marriage_test], axis=1) #same for test_data\n",
    "train_label = train_data.iloc[:,-1] #recall that this is the training data from .csv, we are trying to get the labels which is the last column (hence why -1 is used)\n",
    "test_label = test_data.iloc[:,-1] #same for testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We are finally ready to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression() #first, we create an instance of LogisiticRegression \n",
    "lg.fit(train_data2, train_label) #now we train our model, using the prepped data\n",
    "pred = lg.predict(test_data2) #this gives all the predictions of our model in the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's import some performance measures to see how our model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix #the imported functions are self explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n",
      "Recall:  0.4745762711864407\n",
      "Precision:  0.6511627906976745\n",
      "Confusion Matrix: \n",
      " [[64 15]\n",
      " [31 28]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(test_label, pred)) #note that all of these functions follow the same format, first arguement is the true labels, second is the predicted labels\n",
    "print('Recall: ', recall_score(test_label, pred))\n",
    "print('Precision: ', precision_score(test_label, pred))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(test_label, pred)) #\\n in 'Confusion Matrix: \\n' means go to next line so that our confusion matrix will look cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's now do some feature selection by adding an extra feature, say NUM_PREV_APP. We just have to copy all the code from above and add in 'NUM_PREV_APP' in selected_features. As a note, if you're going to add a nominal feature, you have to create a new instance of OneHotEncoder, then concatenate it. However, we are adding an ordinal feature this time so there's no need for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8260869565217391\n",
      "Recall:  0.6779661016949152\n",
      "Precision:  0.8888888888888888\n",
      "Confusion Matrix: \n",
      " [[74  5]\n",
      " [19 40]]\n"
     ]
    }
   ],
   "source": [
    "#we skip all the comments and just have the code below\n",
    "#keep in mind we don't need to import the performance metrics again \n",
    "#also, we overwritten encoded_marriage_train with a DataFrame, so there's also no need to copy that\n",
    "selected_features = ['DEBT', 'AGE', 'YRS_OF_EMPLOYMENT', 'NUM_PREV_APP']\n",
    "train_data2 = pd.concat([train_data[selected_features], encoded_marriage_train], axis=1) \n",
    "test_data2 = pd.concat([test_data[selected_features], encoded_marriage_test], axis=1) \n",
    "train_label = train_data.iloc[:,-1] \n",
    "test_label = test_data.iloc[:,-1]\n",
    "lg = LogisticRegression() \n",
    "lg.fit(train_data2, train_label)\n",
    "pred = lg.predict(test_data2) \n",
    "print('Accuracy: ', accuracy_score(test_label, pred)) #note that all of these functions follow the same format, first arguement is the true labels, second is the predicted labels\n",
    "print('Recall: ', recall_score(test_label, pred))\n",
    "print('Precision: ', precision_score(test_label, pred))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(test_label, pred)) #\\n in 'Confusion Matrix: \\n' means go to next line so that our confusion matrix will look cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We see a drastic improvement by adding this feature. We now move on to hyperparameters tuning. \n",
    "###### Logistic Regression in sklearn comes default with a L2 regularizer with a coefficient of 1 by default. What L2 regularizer is suppose to do is to prevent overfitting during training. The exact mechanism for how this happens is beyond the scope of these notes. However, this coefficient is a hyperparameter of Logistic Regression, and is used as an example of how to tune hyperparameters using sklearn's GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, we put all possible hyperparameters in a Python dictionary. A Python dictionary is just another way of storing data. Essentially, it can be viewed as a list of 'keys' and 'values', where a 'key' has a corresponding 'value'. \n",
    "##### LogisticRegression's L2 coefficient is named C in sklearn, so we put 'C' (key). The corresponding 'value' should be all the values we want to test for the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {'C':[i/100 for i in range(1,100)]} #I start at 1 because C can't take on the value 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, we create an instance of GridSearchCV to  find the best hyperparamter. First positional argument should be the instance of the model we are trying to optimize. Second positional argument is the dictionary for hyperparamters. The keyword argument scoring is how the gridsearch will be evaluating the different hyperparameters performance. This case, we are using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(lg, hyperparams, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48....83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.fit(train_data2, train_label) #gridsearch automaticallty makes a validation set from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.62, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_estimator_ #this gives the best version of the model with its corresponding hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keep in mind that we are only optimizing for accuracy. Depending on the needs for a model, one might be tuning hyperparameters for other performance measures. This concludes the sample run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
